---
title: "Installing Slurm on Ubuntu to run AI benchmarks"
categories: [GPU, AI, HPC, Slurm]
badges: true
toc: true
comments: true
layout: post
author: Jean-Louis Queguiner
---

# Context
When running benchmarks you need to find the appropriate library to compete with.
When it comes to AI the standard is [ML commons](https://mlcommons.org) created in 2019 to ease benchmark reproductibility in Machine Learning and AI.

I did a Youtube video regarding this subject explaining ML commons and how benchmarks work:
[![Doing AI GPU Benchmarks with ML commons (Part1)](http://img.youtube.com/vi/L9qbVfQWf_8/0.jpg)](https://www.youtube.com/watch?v=L9qbVfQWf_8 "Doing AI GPU Benchmarks with ML commons (Part1)")


# Slurm to the rescure
When performing benchmarks is often that you want to test the scale of the GPU, then the machine using multiple GPU then the whole system using mutliple Nodes.

This is where distributed computing comes into play with 2 main components:
- the software libs : Torch distributed/horovod 
- the orchestration : slurm

Slurm was introduced in 2010 for scientific High Performance Computing (HPC).

Slurm allow you to run a coordinated workload on multiple workers.

## Installing slurm on Linux / Ubuntu 20.04

```console
# checking OS version
$ cat /etc/os-release
```

```console
# updating package index
$ sudo apt update
```


```console
# installing slurm
$ sudo apt-get install -y slurm-wlm
```

You have now installed :
- slurmctld : the central management daemon of Slurm
- slurmd : the compute node daemon of Slurm

The central management daemon coordinates the jobs, accept the jobs, schedule/allocate the resources and monitor the job status.
The compute node daemon monitors and managed all jobs running on the compute node.

slurmctld is basically the master.
slurmd are basically the workers.

see slurm architecture : 
[Slurm course](http://skutnik.iiens.net/cours/2A/LC/Batch%20Scheduling/Cours%202.pdf)

Both of them should be install in /usr/sbin/

## configuring slurm
We will serve the slurm configurator page to port 8000.
This will allow use to use and HTML interface for configuration that will then create a /etc/slurm-llnl/slurm.config config file.

```console
$ chmod +r $(dpkg -L slurmctld | grep slurm-wlm-configurator.html) &&  \
python3 -m http.server --directory $(dirname $(dpkg -L slurmctld | grep slurm-wlm-configurator.html))
```

```console
# getting the current host name
$ hostnamectl

root@gpu-client-1:~# hostnamectl
   Static hostname: gpu-client-1
         Icon name: computer-server
           Chassis: server
        Machine ID: XXXX
           Boot ID: XXXX
  Operating System: Ubuntu 20.04.3 LTS
            Kernel: Linux 5.4.0-88-generic
      Architecture: x86-64
```

```console
# getting the current host configuration
$ lscpu | grep -E '^Thread|^Core|^Socket|^CPU\('

CPU(s):                          32
Thread(s) per core:              2
Core(s) per socket:              8
Socket(s):                       2
```

```console
# getting the memory.
$ free -m
              total        used        free      shared  buff/cache   available
Mem:         192077        1647       47100           2      143329      189014
Swap:          2047          26        2021
`````

## starting slurm
```console
# start the worker deamon
$ sudo service slurmd start
```

```console
# start the master deamon
$ sudo service slurmctld start
```

need to set the list of host in all nodes in the /etc/hosts

The resolvers must be also

10.100.10.11  hostname for instance.
add private IP of gateway in /etc/hosts

create pk
add pk to ansible playbook

set the defautl /mnt/ share storage

set ntp servers for config/synchro in /cluster/defaults
add nodes in ansible-slurm/environments/prod/hosts
et le group vars

partition = groups of machines with rules => a bit like queue and hardware idealy / same contraints.








