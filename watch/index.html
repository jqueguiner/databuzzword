<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://databuzzword.com/watch/" />
<meta property="og:url" content="https://databuzzword.com/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","headline":"Technology Watch","@type":"WebPage","url":"https://databuzzword.com/watch/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://databuzzword.com/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://databuzzword.com/watch/" />
<meta property="og:url" content="https://databuzzword.com/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","headline":"Technology Watch","@type":"WebPage","url":"https://databuzzword.com/watch/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://databuzzword.com/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">DataBuzzWord Blog!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About The Podcast</a><a class="page-link" href="/bookmarks/">Bookmarks</a><a class="page-link" href="/experimenta/">DataBuzzWord Experiments</a><a class="page-link" href="/reading-list/">Reading List</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a><a class="page-link" href="/watch/">Technology Watch</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Technology Watch</h1>
  </header>

  <div class="post-content">
    <ol id="markdown-toc">
  <li><a href="#machinedeep-learning-tools" id="markdown-toc-machinedeep-learning-tools">Machine/Deep Learning tools</a>    <ol>
      <li><a href="#common-issues" id="markdown-toc-common-issues">Common issues</a>        <ol>
          <li><a href="#unbalanced-dataset" id="markdown-toc-unbalanced-dataset">Unbalanced Dataset</a>            <ol>
              <li><a href="#solution--oversampling" id="markdown-toc-solution--oversampling">Solution : Oversampling</a></li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#technics" id="markdown-toc-technics">Technics</a>        <ol>
          <li><a href="#clustering" id="markdown-toc-clustering">Clustering</a></li>
          <li><a href="#data-labeling" id="markdown-toc-data-labeling">Data Labeling</a></li>
          <li><a href="#reinforcement-learning" id="markdown-toc-reinforcement-learning">Reinforcement Learning</a>            <ol>
              <li><a href="#tensortrade" id="markdown-toc-tensortrade">TensorTrade</a></li>
            </ol>
          </li>
          <li><a href="#text-few-shot-learning" id="markdown-toc-text-few-shot-learning">Text few shot learning</a></li>
          <li><a href="#text-summarization" id="markdown-toc-text-summarization">Text summarization</a></li>
          <li><a href="#transformers" id="markdown-toc-transformers">Transformers</a></li>
          <li><a href="#bert" id="markdown-toc-bert">BERT</a>            <ol>
              <li><a href="#blogs" id="markdown-toc-blogs">Blogs</a></li>
              <li><a href="#bertweet-a-pre-trained-language-model-for-english-tweets" id="markdown-toc-bertweet-a-pre-trained-language-model-for-english-tweets">BERTweet: A pre-trained language model for English Tweets</a></li>
              <li><a href="#covid-twitter-bert" id="markdown-toc-covid-twitter-bert">COVID-Twitter-BERT</a></li>
            </ol>
          </li>
          <li><a href="#gpt2" id="markdown-toc-gpt2">GPT2</a>            <ol>
              <li><a href="#aitextgen--train-a-gpt-2-text-generating-model-w-gpu" id="markdown-toc-aitextgen--train-a-gpt-2-text-generating-model-w-gpu">aitextgen – Train a GPT-2 Text-Generating Model w/ GPU</a></li>
              <li><a href="#relates-blogs" id="markdown-toc-relates-blogs">Relates Blogs</a></li>
            </ol>
          </li>
          <li><a href="#question-answering" id="markdown-toc-question-answering">Question Answering</a></li>
          <li><a href="#reformers" id="markdown-toc-reformers">Reformers</a></li>
        </ol>
      </li>
      <li><a href="#benchmaks" id="markdown-toc-benchmaks">Benchmaks</a>        <ol>
          <li><a href="#text" id="markdown-toc-text">Text</a>            <ol>
              <li><a href="#xglue-expanding-cross-lingual-understanding-and-generation-with-tasks-from-real-world-scenarios" id="markdown-toc-xglue-expanding-cross-lingual-understanding-and-generation-with-tasks-from-real-world-scenarios">XGLUE: Expanding cross-lingual understanding and generation with tasks from real-world scenarios</a></li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#useful-libs" id="markdown-toc-useful-libs">Useful Libs</a>        <ol>
          <li><a href="#wrapper" id="markdown-toc-wrapper">Wrapper</a>            <ol>
              <li><a href="#vision" id="markdown-toc-vision">Vision</a></li>
              <li><a href="#text-1" id="markdown-toc-text-1">Text</a>                <ol>
                  <li><a href="#fastai-code-first-intro-to-natural-language-processing" id="markdown-toc-fastai-code-first-intro-to-natural-language-processing">fast.ai Code-First Intro to Natural Language Processing</a></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><a href="#text-2" id="markdown-toc-text-2">Text</a>            <ol>
              <li><a href="#nltk" id="markdown-toc-nltk">NLTK</a></li>
              <li><a href="#spacy" id="markdown-toc-spacy">SpaCy</a></li>
              <li><a href="#transformers-huggingface" id="markdown-toc-transformers-huggingface">Transformers (HuggingFace)</a>                <ol>
                  <li><a href="#related-blogs" id="markdown-toc-related-blogs">Related blogs</a></li>
                </ol>
              </li>
              <li><a href="#simple-transformers-based-on-huggingface" id="markdown-toc-simple-transformers-based-on-huggingface">Simple Transformers (based on HuggingFace)</a></li>
              <li><a href="#cdqa-unsupervised-qa" id="markdown-toc-cdqa-unsupervised-qa">cdQA: Unsupervised QA</a></li>
              <li><a href="#facebook-unsupervisedqa" id="markdown-toc-facebook-unsupervisedqa">Facebook UnsupervisedQA</a></li>
            </ol>
          </li>
          <li><a href="#others" id="markdown-toc-others">Others</a>            <ol>
              <li><a href="#facebook-mmf" id="markdown-toc-facebook-mmf">Facebook MMF</a></li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#hands-on" id="markdown-toc-hands-on">Hands-on</a>        <ol>
          <li><a href="#nlp" id="markdown-toc-nlp">NLP</a>            <ol>
              <li><a href="#structured-data" id="markdown-toc-structured-data">Structured Data</a></li>
            </ol>
          </li>
          <li><a href="#automl" id="markdown-toc-automl">AutoML</a>            <ol>
              <li><a href="#autokeras" id="markdown-toc-autokeras">AutoKeras</a></li>
              <li><a href="#ovhcloud-automl" id="markdown-toc-ovhcloud-automl">OVHcloud autoML</a></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#deep-learning-use-cases" id="markdown-toc-deep-learning-use-cases">Deep Learning use cases</a>    <ol>
      <li><a href="#nothing-to-image" id="markdown-toc-nothing-to-image">Nothing to Image</a>        <ol>
          <li><a href="#generative" id="markdown-toc-generative">Generative</a>            <ol>
              <li><a href="#face" id="markdown-toc-face">Face</a>                <ol>
                  <li><a href="#disentangled-image-generation-through-structured-noise-injection" id="markdown-toc-disentangled-image-generation-through-structured-noise-injection">Disentangled Image Generation Through Structured Noise Injection</a></li>
                </ol>
              </li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#image-to-anything" id="markdown-toc-image-to-anything">Image to Anything</a>        <ol>
          <li><a href="#image-to-image" id="markdown-toc-image-to-image">Image to Image</a>            <ol>
              <li><a href="#avatarify--deepdake-for-zoom" id="markdown-toc-avatarify--deepdake-for-zoom">Avatarify : Deepdake for zoom</a></li>
              <li><a href="#inpainting" id="markdown-toc-inpainting">Inpainting</a>                <ol>
                  <li><a href="#high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling" id="markdown-toc-high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling">High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling</a></li>
                  <li><a href="#edgeconnect-generative-image-inpainting-with-adversarial-edge-learning" id="markdown-toc-edgeconnect-generative-image-inpainting-with-adversarial-edge-learning">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</a></li>
                  <li><a href="#progressive-image-inpainting-with-full-resolution-residual-network" id="markdown-toc-progressive-image-inpainting-with-full-resolution-residual-network">Progressive Image Inpainting with Full-Resolution Residual Network</a></li>
                </ol>
              </li>
              <li><a href="#super-resolution" id="markdown-toc-super-resolution">Super resolution</a>                <ol>
                  <li><a href="#pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models" id="markdown-toc-pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</a></li>
                  <li><a href="#image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining" id="markdown-toc-image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</a></li>
                </ol>
              </li>
              <li><a href="#image-to-image-translation" id="markdown-toc-image-to-image-translation">Image-to-Image translation</a>                <ol>
                  <li><a href="#deepfacedrawing-deep-generation-of-face-images-from-sketches" id="markdown-toc-deepfacedrawing-deep-generation-of-face-images-from-sketches">DeepFaceDrawing: Deep Generation of Face Images from Sketches</a></li>
                  <li><a href="#ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020" id="markdown-toc-ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020">UGATIT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)</a></li>
                  <li><a href="#selfie-to-anime" id="markdown-toc-selfie-to-anime">Selfie to Anime</a></li>
                </ol>
              </li>
              <li><a href="#segmentation" id="markdown-toc-segmentation">Segmentation</a>                <ol>
                  <li><a href="#poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3" id="markdown-toc-poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3">Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3</a></li>
                  <li><a href="#attention-guided-hierarchical-structure-aggregation-for-image-matting" id="markdown-toc-attention-guided-hierarchical-structure-aggregation-for-image-matting">Attention-Guided Hierarchical Structure Aggregation for Image Matting</a></li>
                  <li><a href="#foreground-aware-semantic-representations-for-image-harmonization" id="markdown-toc-foreground-aware-semantic-representations-for-image-harmonization">Foreground-aware Semantic Representations for Image Harmonization</a></li>
                  <li><a href="#single-stage-semantic-segmentation-from-image-labels-cvpr-2020" id="markdown-toc-single-stage-semantic-segmentation-from-image-labels-cvpr-2020">Single-Stage Semantic Segmentation from Image Labels (CVPR 2020)</a></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><a href="#others-1" id="markdown-toc-others-1">Others</a>            <ol>
              <li><a href="#background-matting-the-world-is-your-green-screen" id="markdown-toc-background-matting-the-world-is-your-green-screen">Background Matting: The World is Your Green Screen</a></li>
              <li><a href="#3d-photography-using-context-aware-layered-depth-inpainting-cvpr-2020" id="markdown-toc-3d-photography-using-context-aware-layered-depth-inpainting-cvpr-2020">3D Photography using Context-aware Layered Depth Inpainting (CVPR 2020)</a></li>
              <li><a href="#project-an-image-centroid-to-another-image-using-opencv" id="markdown-toc-project-an-image-centroid-to-another-image-using-opencv">Project an image centroid to another image using OpenCV</a></li>
            </ol>
          </li>
          <li><a href="#image-to-text" id="markdown-toc-image-to-text">Image to Text</a>            <ol>
              <li><a href="#compguesswhat-a-multi-task-evaluation-framework-for-grounded-language-learning" id="markdown-toc-compguesswhat-a-multi-task-evaluation-framework-for-grounded-language-learning">CompGuessWhat?!: a Multi-Task Evaluation Framework for Grounded Language Learning</a></li>
              <li><a href="#yolov4-optimal-speed-and-accuracy-of-object-detection" id="markdown-toc-yolov4-optimal-speed-and-accuracy-of-object-detection">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></li>
              <li><a href="#image-captioning-with-pytorch" id="markdown-toc-image-captioning-with-pytorch">Image Captioning with PyTorch</a></li>
              <li><a href="#resnest-split-attention-networks" id="markdown-toc-resnest-split-attention-networks">ResNeSt: Split-Attention Networks</a></li>
              <li><a href="#hands-on-guide-to-sign-language-classification-using-cnn" id="markdown-toc-hands-on-guide-to-sign-language-classification-using-cnn">Hands-on guide to sign language classification using CNN</a></li>
            </ol>
          </li>
          <li><a href="#image-to-soundspeech" id="markdown-toc-image-to-soundspeech">Image to Sound/Speech</a></li>
          <li><a href="#image-to-video" id="markdown-toc-image-to-video">Image to Video</a></li>
        </ol>
      </li>
      <li><a href="#text-to-anything" id="markdown-toc-text-to-anything">Text to Anything</a>        <ol>
          <li><a href="#text-to-image" id="markdown-toc-text-to-image">Text to Image</a>            <ol>
              <li><a href="#network-fusion-for-content-creation-with-conditional-inns-cvpr-2020" id="markdown-toc-network-fusion-for-content-creation-with-conditional-inns-cvpr-2020">Network Fusion for Content Creation with Conditional INNs (CVPR 2020)</a></li>
            </ol>
          </li>
          <li><a href="#text-to-text" id="markdown-toc-text-to-text">Text to Text</a>            <ol>
              <li><a href="#bilingual-translation" id="markdown-toc-bilingual-translation">Bilingual Translation</a></li>
              <li><a href="#t5-finetuning" id="markdown-toc-t5-finetuning">T5 finetuning</a></li>
              <li><a href="#training-electra" id="markdown-toc-training-electra">Training Electra</a></li>
              <li><a href="#text-translation" id="markdown-toc-text-translation">Text Translation</a></li>
              <li><a href="#text-generation" id="markdown-toc-text-generation">Text Generation</a>                <ol>
                  <li><a href="#lyrics-generation" id="markdown-toc-lyrics-generation">Lyrics Generation</a></li>
                  <li><a href="#next-word-prediction" id="markdown-toc-next-word-prediction">Next Word Prediction</a></li>
                </ol>
              </li>
              <li><a href="#code-to-code" id="markdown-toc-code-to-code">Code to Code</a>                <ol>
                  <li><a href="#unsupervised-translation-of-programming-languages" id="markdown-toc-unsupervised-translation-of-programming-languages">Unsupervised Translation of Programming Languages</a></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><a href="#text-to-soundspeech" id="markdown-toc-text-to-soundspeech">Text to Sound/Speech</a>            <ol>
              <li><a href="#pitchtron-towards-audiobook-generation-from-ordinary-peoples-voices" id="markdown-toc-pitchtron-towards-audiobook-generation-from-ordinary-peoples-voices">Pitchtron: Towards audiobook generation from ordinary people’s voices</a></li>
              <li><a href="#transformers-tts" id="markdown-toc-transformers-tts">Transformers TTS</a></li>
            </ol>
          </li>
          <li><a href="#text-to-video" id="markdown-toc-text-to-video">Text to Video</a></li>
        </ol>
      </li>
      <li><a href="#soundspeech-to-anything" id="markdown-toc-soundspeech-to-anything">Sound/Speech to Anything</a>        <ol>
          <li><a href="#soundspeech-to-image" id="markdown-toc-soundspeech-to-image">Sound/Speech to Image</a>            <ol>
              <li><a href="#audio-to-image-conversion" id="markdown-toc-audio-to-image-conversion">Audio to Image Conversion</a></li>
            </ol>
          </li>
          <li><a href="#soundspeech-to-text" id="markdown-toc-soundspeech-to-text">Sound/Speech to Text</a>            <ol>
              <li><a href="#speech-command-recognition" id="markdown-toc-speech-command-recognition">Speech Command Recognition</a></li>
            </ol>
          </li>
          <li><a href="#soundspeech-to-soundspeech" id="markdown-toc-soundspeech-to-soundspeech">Sound/Speech to Sound/Speech</a>            <ol>
              <li><a href="#speaker-independent-emotional-voice-conversion-based-on-conditional-vaw-gan-and-cwt" id="markdown-toc-speaker-independent-emotional-voice-conversion-based-on-conditional-vaw-gan-and-cwt">Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT</a></li>
            </ol>
          </li>
          <li><a href="#soundspeech-to-video" id="markdown-toc-soundspeech-to-video">Sound/Speech to Video</a></li>
        </ol>
      </li>
      <li><a href="#video-to-anything" id="markdown-toc-video-to-anything">Video to Anything</a>        <ol>
          <li><a href="#video-to-video" id="markdown-toc-video-to-video">Video to Video</a>            <ol>
              <li><a href="#segmentation-1" id="markdown-toc-segmentation-1">Segmentation</a>                <ol>
                  <li><a href="#mseg--a-composite-dataset-for-multi-domain-semantic-segmentation" id="markdown-toc-mseg--a-composite-dataset-for-multi-domain-semantic-segmentation">MSeg : A Composite Dataset for Multi-domain Semantic Segmentation</a></li>
                  <li><a href="#motion-supervised-co-part-segmentation" id="markdown-toc-motion-supervised-co-part-segmentation">Motion Supervised co-part Segmentation</a></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><a href="#video-to-image" id="markdown-toc-video-to-image">Video to Image</a></li>
          <li><a href="#video-to-text" id="markdown-toc-video-to-text">Video to Text</a></li>
          <li><a href="#video-tosoundspeech" id="markdown-toc-video-tosoundspeech">Video toSound/Speech</a></li>
          <li><a href="#video-to-video-1" id="markdown-toc-video-to-video-1">Video to Video</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#inference" id="markdown-toc-inference">Inference</a>    <ol>
      <li><a href="#python-serving" id="markdown-toc-python-serving">Python serving</a></li>
      <li><a href="#fastai" id="markdown-toc-fastai">Fastai</a></li>
      <li><a href="#huggingface" id="markdown-toc-huggingface">HuggingFace</a></li>
      <li><a href="#hummingbird" id="markdown-toc-hummingbird">Hummingbird</a></li>
    </ol>
  </li>
  <li><a href="#tools" id="markdown-toc-tools">Tools</a>    <ol>
      <li><a href="#terminal" id="markdown-toc-terminal">Terminal</a>        <ol>
          <li><a href="#rich" id="markdown-toc-rich">Rich</a></li>
        </ol>
      </li>
      <li><a href="#python" id="markdown-toc-python">Python</a>        <ol>
          <li><a href="#pyaudio-fft" id="markdown-toc-pyaudio-fft">PyAudio FFT</a></li>
          <li><a href="#process-mining--alpha-miner" id="markdown-toc-process-mining--alpha-miner">Process Mining : alpha-miner</a></li>
          <li><a href="#image-feature-extractor" id="markdown-toc-image-feature-extractor">Image Feature extractor</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#cool-projects" id="markdown-toc-cool-projects">Cool projects</a>    <ol>
      <li><a href="#web-based-training" id="markdown-toc-web-based-training">Web based Training</a></li>
      <li><a href="#how-to-evaluate-longformer-on-triviaqa-using-nlp" id="markdown-toc-how-to-evaluate-longformer-on-triviaqa-using-nlp">How to evaluate Longformer on TriviaQA using NLP</a></li>
      <li><a href="#data-visualization" id="markdown-toc-data-visualization">Data Visualization</a></li>
    </ol>
  </li>
  <li><a href="#hardware" id="markdown-toc-hardware">Hardware</a>    <ol>
      <li><a href="#gpu" id="markdown-toc-gpu">GPU</a>        <ol>
          <li><a href="#nvidia" id="markdown-toc-nvidia">Nvidia</a>            <ol>
              <li><a href="#ampere" id="markdown-toc-ampere">Ampere</a></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#mooc" id="markdown-toc-mooc">MOOC</a>    <ol>
      <li><a href="#fastai-1" id="markdown-toc-fastai-1">Fast.ai</a></li>
      <li><a href="#benchmark" id="markdown-toc-benchmark">Benchmark</a>        <ol>
          <li><a href="#nlp-1" id="markdown-toc-nlp-1">NLP</a></li>
        </ol>
      </li>
    </ol>
  </li>
</ol>
<h1 id="machinedeep-learning-tools">Machine/Deep Learning tools</h1>
<h2 id="common-issues">Common issues</h2>
<h3 id="unbalanced-dataset">Unbalanced Dataset</h3>
<h4 id="solution--oversampling">Solution : Oversampling</h4>
<ul>
  <li><a href="https://www.kaggle.com/tanlikesmath/oversampling-mnist-with-fastai">Oversampling with FastAI</a></li>
</ul>

<h2 id="technics">Technics</h2>

<h3 id="clustering">Clustering</h3>
<ul>
  <li><a href="https://towardsdatascience.com/understanding-k-means-k-means-and-k-medoids-clustering-algorithms-ad9c9fbf47ca">Understanding K-Means, K-Means++ and, K-Medoids Clustering Algorithms</a></li>
  <li><a href="https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html">Centroid Initialization Methods for k-means Clustering</a></li>
</ul>

<h3 id="data-labeling">Data Labeling</h3>
<ul>
  <li><a href="https://medium.com/@lessw/reducing-your-labeled-data-requirements-2-5x-for-deep-learning-google-brains-new-contrastive-2ac0da0367ef">Reducing your labeled data requirements (2–5x) for Deep Learning: Deep Mind’s new “Contrastive Predictive Coding 2.0”</a></li>
  <li><a href="https://machinelearningtokyo.com/2020/06/04/annotation-tools-for-computer-vision-and-nlp/amp/?__twitter_impression=true">Annotation tools for computer vision and NLP</a></li>
</ul>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<h4 id="tensortrade">TensorTrade</h4>
<ul>
  <li><a href="https://github.com/tensortrade-org/tensortrade/blob/master/README.md">Code</a></li>
</ul>

<h3 id="text-few-shot-learning">Text few shot learning</h3>
<ul>
  <li><a href="https://joeddav.github.io/blog/2020/05/29/ZSL.html">Zero shot learning</a></li>
</ul>

<h3 id="text-summarization">Text summarization</h3>
<p>Great tutorial serie here</p>
<ul>
  <li><a href="https://medium.com/hackernoon/text-summarizer-using-deep-learning-made-easy-490880df6cd">Part 1</a></li>
  <li><a href="https://medium.com/hackernoon/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46">Part 2</a></li>
  <li><a href="https://medium.com/hackernoon/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0">Part 3</a></li>
  <li><a href="https://medium.com/hackernoon/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">Part 4</a></li>
  <li><a href="https://medium.com/hackernoon/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">Part 5</a></li>
  <li><a href="https://medium.com/hackernoon/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55">Part 6</a></li>
  <li><a href="https://medium.com/hackernoon/combination-of-abstractive-extractive-methods-for-text-summarization-tutorial-7-8a4fb85d67e2">Part 7</a></li>
  <li><a href="https://medium.com/hackernoon/teach-seq2seq-models-to-learn-from-their-mistakes-using-deep-curriculum-learning-tutorial-8-a730a387754">Part 8</a></li>
  <li><a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c">Part 9</a></li>
  <li><a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c">Part 10</a></li>
  <li><a href="https://github.com/theamrzaki/text_summurization_abstractive_methods">Code</a></li>
</ul>

<h3 id="transformers">Transformers</h3>
<ul>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">The illustrated Transformers</a></li>
  <li><a href="https://arxiv.org/abs/2001.04451">Transformers explained</a></li>
  <li><a href="https://arxiv.org/abs/1706.03762">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>
<ul>
  <li><a href="https://forums.fast.ai/t/huggingface-nlp-create-fastai-dataloaders-show-batch-and-create-dataset-for-lm-mlm/73179">HuggingFace/nlp - Create fastai Dataloaders, show batch, and create dataset for LM, MLM</a></li>
</ul>

<h3 id="bert">BERT</h3>
<h4 id="blogs">Blogs</h4>
<ul>
  <li><a href="https://app.wandb.ai/cayush/bert-finetuning/reports/Sentence-classification-with-Huggingface-BERT-and-W%26B--Vmlldzo4MDMwNA">Blog Text Classification using Transformers</a></li>
  <li><a href="https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks">Community Notebooks</a></li>
</ul>

<h4 id="bertweet-a-pre-trained-language-model-for-english-tweets">BERTweet: A pre-trained language model for English Tweets</h4>
<ul>
  <li><a href="https://github.com/VinAIResearch/BERTweet/blob/master/README.md#models2">Code</a></li>
</ul>

<h4 id="covid-twitter-bert">COVID-Twitter-BERT</h4>
<ul>
  <li><a href="https://github.com/digitalepidemiologylab/covid-twitter-bert">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.07503">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{muller2020covid,
  title={COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter},
  author={M{\"u}ller, Martin and Salath{\'e}, Marcel and Kummervold, Per E},
  journal={arXiv preprint arXiv:2005.07503},
  year={2020}
}
</code></pre></div></div>

<h3 id="gpt2">GPT2</h3>

<h4 id="aitextgen--train-a-gpt-2-text-generating-model-w-gpu">aitextgen – Train a GPT-2 Text-Generating Model w/ GPU</h4>
<ul>
  <li><a href="https://github.com/minimaxir/aitextgen">Code</a></li>
  <li><a href="https://docs.aitextgen.io/">Doc</a></li>
  <li><a href="https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing">Sample</a></li>
  <li><a href="https://twitter.com/minimaxir">Author</a></li>
</ul>

<h4 id="relates-blogs">Relates Blogs</h4>
<ul>
  <li><a href="https://lvwerra.github.io/trl/04-gpt2-sentiment-ppo-training/">Tune GPT2 to generate positive reviews</a></li>
</ul>

<h3 id="question-answering">Question Answering</h3>
<ul>
  <li><a href="https://medium.com/illuin/deep-learning-has-almost-all-the-answers-yes-no-question-answering-with-transformers-223bebb70189">Deep Learning has (almost) all the answers: Yes/No Question Answering with Transformers</a></li>
  <li><a href="https://medium.com/illuin/building-an-open-domain-question-answering-pipeline-in-french-97304e63c369">Building a simple Open Domain Question Answering pipeline in French</a></li>
</ul>

<h3 id="reformers">Reformers</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2001.04451">Colab</a></li>
</ul>

<h2 id="benchmaks">Benchmaks</h2>
<h3 id="text">Text</h3>
<h4 id="xglue-expanding-cross-lingual-understanding-and-generation-with-tasks-from-real-world-scenarios">XGLUE: Expanding cross-lingual understanding and generation with tasks from real-world scenarios</h4>
<p><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/06/XGLUE-homepage-feat-image-800x550.png" alt="" /></p>
<ul>
  <li><a href="https://www.microsoft.com/en-us/research/blog/xglue-expanding-cross-lingual-understanding-and-generation-with-tasks-from-real-world-scenarios/">Blog</a></li>
  <li><a href="https://github.com/microsoft/Unicoder">Code</a></li>
  <li><a href="https://github.com/microsoft/XGLUE">Dataset</a></li>
  <li><a href="https://www.microsoft.com/en-us/research/publication/xglue-a-new-benchmark-dataset-for-cross-lingual-pre-training-understanding-and-generation/">Paper</a></li>
</ul>

<h2 id="useful-libs">Useful Libs</h2>
<h3 id="wrapper">Wrapper</h3>
<h4 id="vision">Vision</h4>
<h4 id="text-1">Text</h4>
<h5 id="fastai-code-first-intro-to-natural-language-processing">fast.ai Code-First Intro to Natural Language Processing</h5>
<ul>
  <li><a href="https://www.fast.ai/2019/07/08/fastai-nlp/">Project Page</a></li>
  <li><a href="https://github.com/fastai/course-nlp">Code</a></li>
</ul>

<p>here is the associated tutorial serie:</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<ul>
  <li><a href="https://www.ntentional.com/nlp/training%20technique/classification/2020/04/17/fasthugs_seq_classification.html">FastAI + HuggingFace = FastHugs</a></li>
</ul>

<h3 id="text-2">Text</h3>
<h4 id="nltk">NLTK</h4>
<ul>
  <li><a href="https://www.nltk.org">Project Page</a></li>
  <li><a href="https://github.com/nltk/nltk">Code</a></li>
  <li><a href="https://www.nltk.org/index.html">Doc</a></li>
  <li><a href="https://cheatography.com/murenei/cheat-sheets/natural-language-processing-with-python-and-nltk/">Cheat Sheet</a></li>
</ul>

<p>I consider <a href="https://twitter.com/Sentdex">@SentDex</a> founder <a href="https://pythonprogramming.net">pythonprogramming.net</a> and <a href="https://www.youtube.com/channel/sentdex">https://www.youtube.com/channel/sentdex</a> as the best tutorial for NLTK</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLI142kNg_e0Q57BmOF9H4UnXiWNSVZZ-O" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="spacy">SpaCy</h4>
<ul>
  <li><a href="https://spacy.io/">Project Page</a></li>
  <li><a href="https://github.com/explosion/spaCy">Code</a></li>
  <li><a href="https://spacy.io/api/doc">Doc</a></li>
  <li><a href="https://www.datacamp.com/community/blog/spacy-cheatsheet">Cheat Sheet</a></li>
  <li><a href="https://twitter.com/spacy_io">Spacy on Twitter @spacy_io</a></li>
  <li><a href="https://www.linkedin.com/company/explosion-ai/">Spacy on Linkedin</a></li>
  <li><a href="https://www.youtube.com/c/ExplosionAI">Spacy on Youtube</a></li>
</ul>

<p>A good Spacy tutorial Youtube serie here :</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLJ39kWiJXSiz1LK8d_fyxb7FTn4mBYOsD" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Spacy channel :</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLBmcuObd5An559HbDr_alBnwVsGq-7uTF" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="transformers-huggingface">Transformers (HuggingFace)</h4>
<ul>
  <li><a href="https://huggingface.co/">Project Page</a></li>
  <li><a href="https://github.com/huggingface/transformers">Code</a></li>
  <li><a href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb">Colab</a></li>
  <li><a href="https://huggingface.co/transformers/">Doc</a></li>
  <li><a href="https://twitter.com/huggingface">HuggingFace on Twitter @HuggingFace</a></li>
  <li><a href="https://www.linkedin.com/company/huggingface/">HuggingFace on Linkedin</a></li>
</ul>

<h5 id="related-blogs">Related blogs</h5>
<ul>
  <li><a href="https://app.wandb.ai/jxmorris12/huggingface-demo/reports/A-Step-by-Step-Guide-to-Tracking-Hugging-Face-Model-Performance--VmlldzoxMDE2MTU">A Step by Step Guide to Tracking Hugging Face Model Performance</a></li>
  <li><a href="https://app.wandb.ai/jack-morris/david-vs-goliath/reports/Does-model-size-matter%3F-A-comparison-of-BERT-and-DistilBERT--VmlldzoxMDUxNzU">Does model size matter? A comparison of BERT and DistilBERT</a></li>
  <li><a href="https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html">Open-Dialog Chatbots for Learning New Languages</a></li>
</ul>

<h4 id="simple-transformers-based-on-huggingface">Simple Transformers (based on HuggingFace)</h4>
<ul>
  <li><a href="https://simpletransformers.ai">Project Page</a></li>
  <li><a href="https://github.com/ThilinaRajapakse/simpletransformers">Code</a></li>
  <li><a href="https://simpletransformers.ai/docs/installation/">Doc</a></li>
  <li><a href="https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3">Blog Post</a></li>
</ul>

<p>Simple Transformers is a wrapper on top of HuggingFace’s Transformer Library take makes it easy to setup and use, here is an example of binary classification :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">simpletransformers.classification</span> <span class="kn">import</span> <span class="n">ClassificationModel</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">logging</span>


<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">transformers_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s">"transformers"</span><span class="p">)</span>
<span class="n">transformers_logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

<span class="c1"># Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.
</span><span class="n">train_data</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'Example sentence belonging to class 1'</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s">'Example sentence belonging to class 0'</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'Example eval sentence belonging to class 1'</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s">'Example eval sentence belonging to class 0'</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">eval_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>

<span class="c1"># Create a ClassificationModel
</span><span class="n">model</span> <span class="o">=</span> <span class="n">ClassificationModel</span><span class="p">(</span><span class="s">'roberta'</span><span class="p">,</span> <span class="s">'roberta-base'</span><span class="p">)</span> <span class="c1"># You can set class weights by using the optional weight argument
</span>
<span class="c1"># Train the model
</span><span class="n">model</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>

<span class="c1"># Evaluate the model
</span><span class="n">result</span><span class="p">,</span> <span class="n">model_outputs</span><span class="p">,</span> <span class="n">wrong_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval_model</span><span class="p">(</span><span class="n">eval_df</span><span class="p">)</span>

</code></pre></div></div>

<h4 id="cdqa-unsupervised-qa">cdQA: Unsupervised QA</h4>
<ul>
  <li><a href="https://cdqa-suite.github.io/cdQA-website/">Project Page</a></li>
  <li><a href="https://github.com/cdqa-suite/cdQA">Code</a></li>
</ul>

<h4 id="facebook-unsupervisedqa">Facebook UnsupervisedQA</h4>
<ul>
  <li><a href="https://github.com/facebookresearch/UnsupervisedQA">Code</a></li>
  <li><a href="https://medium.com/illuin/unsupervised-question-answering-4758e5f2be9b">Blog</a></li>
</ul>

<h3 id="others">Others</h3>
<h4 id="facebook-mmf">Facebook MMF</h4>
<p>A modular framework for vision &amp; language multimodal research from Facebook AI Research (FAIR)
<img src="https://camo.githubusercontent.com/fa3fcc9fb23c9d5e4ba8a7a822c15d53dc892ef7/68747470733a2f2f692e696d6775722e636f6d2f42503873596e6b2e6a7067" alt="" /></p>
<ul>
  <li><a href="https://github.com/facebookresearch/mmf">Code</a></li>
  <li><a href="https://mmf.readthedocs.io/en/latest/">Doc</a></li>
  <li><a href="http://learningsys.org/nips18/assets/papers/35CameraReadySubmissionPythia___A_platform_for_vision_language_multi_modal_research.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{singh2018pythia,
  title={Pythia-a platform for vision \&amp; language research},
  author={Singh, Amanpreet and Goswami, Vedanuj and Natarajan, Vivek and Jiang, Yu and Chen, Xinlei and Shah, Meet and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  booktitle={SysML Workshop, NeurIPS},
  volume={2018},
  year={2018}
}
</code></pre></div></div>

<h2 id="hands-on">Hands-on</h2>
<h3 id="nlp">NLP</h3>
<ul>
  <li><a href="https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb">Modern NLP in Python</a></li>
</ul>

<p>| Tool | Binary Classification | Multi-Label Classification | Question Answering | Tokenization | Generation | Named Entity Recognition |
|-|-|-|-|-|-|-|</p>

<h4 id="structured-data">Structured Data</h4>
<h3 id="automl">AutoML</h3>
<h4 id="autokeras">AutoKeras</h4>

<p><a href="https://autokeras.com">AutoKeras</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>

<span class="kn">import</span> <span class="nn">autokeras</span> <span class="k">as</span> <span class="n">ak</span>

<span class="c1"># Prepare the dataset.
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 28, 28)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000,)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>  <span class="c1"># array([7, 2, 1], dtype=uint8)
</span>
<span class="c1"># Initialize the ImageClassifier.
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">ak</span><span class="o">.</span><span class="n">ImageClassifier</span><span class="p">(</span><span class="n">max_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Search for the best model.
</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># Evaluate on the testing data.
</span><span class="k">print</span><span class="p">(</span><span class="s">'Accuracy: {accuracy}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">accuracy</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{jin2019auto,
  title={Auto-Keras: An Efficient Neural Architecture Search System},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
  pages={1946--1956},
  year={2019},
  organization={ACM}
}
</code></pre></div></div>

<h4 id="ovhcloud-automl">OVHcloud autoML</h4>
<p><img src="https://labs.ovh.com/sites/default/files/inline-images/upload_source_1.png| width=250" alt="Demo" />
<img src="https://labs.ovh.com/sites/default/files/inline-images/optimization_result_2.png | width=250" alt="Demo2" />
<img src="https://labs.ovh.com/sites/default/files/inline-images/model_kpi_2.png | width=250" alt="Demo3" /></p>

<ul>
  <li><a href="https://labs.ovh.com/machine-learning-platform">Site</a></li>
  <li><a href="https://github.com/ovh/prescience-client">Code</a></li>
  <li><a href="https://gitter.im/ovh/ai">Forum</a></li>
</ul>

<h1 id="deep-learning-use-cases">Deep Learning use cases</h1>

<h2 id="nothing-to-image">Nothing to Image</h2>
<h3 id="generative">Generative</h3>
<h4 id="face">Face</h4>
<h5 id="disentangled-image-generation-through-structured-noise-injection">Disentangled Image Generation Through Structured Noise Injection</h5>
<p><img src="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png?raw=true | width=250" alt="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png" /></p>
<ul>
  <li><a href="https://github.com/yalharbi/StructuredNoiseInjection">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.12411">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{alharbi2020disentangled,
    title={Disentangled Image Generation Through Structured Noise Injection},
    author={Yazeed Alharbi and Peter Wonka},
    year={2020},
    eprint={2004.12411},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/7h-7wso9E0k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="image-to-anything">Image to Anything</h2>

<h3 id="image-to-image">Image to Image</h3>
<h4 id="avatarify--deepdake-for-zoom">Avatarify : Deepdake for zoom</h4>
<ul>
  <li><a href="https://github.com/alievk/avatarify">Code</a></li>
  <li><a href="https://github.com/DashBarkHuss/100-days-of-code/blob/master/post-log.md#avatarify-1">Blog</a></li>
</ul>

<h4 id="inpainting">Inpainting</h4>
<h5 id="high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling">High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling</h5>
<p><img src="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg | width=250" alt="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg" /></p>
<ul>
  <li><a href="https://zengxianyu.github.io/iic/">Project Page</a></li>
  <li><a href="http://47.57.135.203:2333/">APP</a></li>
  <li><a href="https://arxiv.org/abs/2005.11742">Paper</a></li>
</ul>

<h5 id="edgeconnect-generative-image-inpainting-with-adversarial-edge-learning">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</h5>
<p><img src="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png | width=250" alt="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/edge-connect">Code</a></li>
  <li><a href="https://arxiv.org/abs/1901.00212">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{nazeri2019edgeconnect,
  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},
  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},
  journal={arXiv preprint},
  year={2019},
}
</code></pre></div></div>

<h5 id="progressive-image-inpainting-with-full-resolution-residual-network">Progressive Image Inpainting with Full-Resolution Residual Network</h5>
<p><img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_damaged2.png?raw=true | width=250" alt="Before" />
<img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_final2.png?raw=true | width=250" alt="After" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/Inpainting_FRRN">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10478">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{guo2019progressive,
    title={Progressive Image Inpainting with Full-Resolution Residual Network},
    author={Zongyu Guo and Zhibo Chen and Tao Yu and Jiale Chen and Sen Liu},
    year={2019},
    eprint={1907.10478},
    archivePrefix={arXiv},
    primaryClass={eess.IV}
}
</code></pre></div></div>

<h4 id="super-resolution">Super resolution</h4>
<h5 id="pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</h5>
<p><img src="http://pulse.cs.duke.edu/assets/094.jpeg | width=250" alt="http://pulse.cs.duke.edu/assets/094.jpeg" /></p>
<ul>
  <li><a href="PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models">Project Page</a></li>
  <li><a href="https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true">Colab</a></li>
  <li><a href="https://github.com/adamian98/pulse">Code</a></li>
  <li><a href="https://arxiv.org/abs/2003.03808">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{PULSE_CVPR_2020, 
author = {Menon, Sachit and Damian, Alex and Hu, McCourt and Ravi, Nikhil and Rudin, Cynthia}, 
title = {PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models}, 
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
month = {June}, 
year = {2020} 
}
</code></pre></div></div>

<h5 id="image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</h5>
<p><img src="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png | width=250" alt="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png" /></p>
<ul>
  <li><a href="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.01424">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Mei2020image,
  title={Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining},
  author={Mei, Yiqun and Fan, Yuchen and Zhou, Yuqian and Huang, Lichao and Huang, Thomas S and Shi, Humphrey},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}
</code></pre></div></div>

<h4 id="image-to-image-translation">Image-to-Image translation</h4>
<h5 id="deepfacedrawing-deep-generation-of-face-images-from-sketches">DeepFaceDrawing: Deep Generation of Face Images from Sketches</h5>
<p><img src="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg | width=250" alt="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg" /></p>
<ul>
  <li><a href="http://geometrylearning.com/DeepFaceDrawing/">Paper</a></li>
</ul>

<iframe width="50%" src="https://www.youtube.com/embed/HSunooUTwKs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020">UGATIT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)</h5>
<p><img src="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true | width=250" alt="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/taki0112/UGATIT#paper--official-pytorch-code">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10830">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
Kim2020U-GAT-IT:,
title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlZ5ySKPH}
}
</code></pre></div></div>

<h5 id="selfie-to-anime">Selfie to Anime</h5>
<p><img src="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg?raw=true | width=250" alt="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg" /></p>
<ul>
  <li><a href="https://selfie2anime.com/">Project Page</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime">Code</a></li>
  <li><a href="https://market-place.ai.ovh.net/#!/apis/59a0426c-c148-4cff-a042-6cc148fcffa5/pages/06641de1-1b1c-4bd2-a41d-e11b1c3bd230">API</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime/blob/master/assets/Deploying-Models-to-the-Masses.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{kim2019ugatit,
    title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
    author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwanghee Lee},
    year={2019},
    eprint={1907.10830},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<ul>
  <li><a href="https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107">Author’s Site</a></li>
</ul>

<h4 id="segmentation">Segmentation</h4>
<h5 id="poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3">Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3</h5>
<p><img src="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false | width=250" alt="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false" /></p>
<ul>
  <li><a href="https://gitlab.com/irafm-ai/poly-yolo">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.13243">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{hurtik2020polyyolo,
    title={Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3},
    author={Petr Hurtik and Vojtech Molek and Jan Hula and Marek Vajgl and Pavel Vlasanek and Tomas Nejezchleba},
    year={2020},
    eprint={2005.13243},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/2KxNnEV-Zes" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="attention-guided-hierarchical-structure-aggregation-for-image-matting">Attention-Guided Hierarchical Structure Aggregation for Image Matting</h5>
<p><img src="https://wukaoliu.github.io/HAttMatting/figures/visualization.png?raw=true | width=250" alt="https://wukaoliu.github.io/HAttMatting/figures/visualization.png" /></p>
<ul>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Project Page</a></li>
  <li><a href="https://github.com/wukaoliu/CVPR2020-HAttMatting">Code</a></li>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Qiao_2020_CVPR,
    author = {Qiao, Yu and Liu, Yuhao and Yang, Xin and Zhou, Dongsheng and Xu, Mingliang and Zhang, Qiang and Wei, Xiaopeng},
    title = {Attention-Guided Hierarchical Structure Aggregation for Image Matting},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}
</code></pre></div></div>

<h5 id="foreground-aware-semantic-representations-for-image-harmonization">Foreground-aware Semantic Representations for Image Harmonization</h5>
<p><img src="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg?raw=true | width=250" alt="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg" /></p>
<ul>
  <li><a href="https://github.com/saic-vul/image_harmonization">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.00809">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{sofiiuk2020harmonization,
  title={Foreground-aware Semantic Representations for Image Harmonization},
  author={Konstantin Sofiiuk, Polina Popenova, Anton Konushin},
  journal={arXiv preprint arXiv:2006.00809},
  year={2020}
}
</code></pre></div></div>

<h5 id="single-stage-semantic-segmentation-from-image-labels-cvpr-2020">Single-Stage Semantic Segmentation from Image Labels (CVPR 2020)</h5>
<p><img src="https://github.com/visinf/1-stage-wseg/blob/master/figures/results.gif?raw=true | width=250" alt="https://github.com/visinf/1-stage-wseg/blob/master/figures/results.gif?raw=true" /></p>
<ul>
  <li><a href="https://github.com/visinf/1-stage-wseg">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.08104">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Araslanov:2020:WSEG,
  title     = {Single-Stage Semantic Segmentation from Image Labels},
  author    = {Araslanov, Nikita and and Roth, Stefan},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2020}
}
</code></pre></div></div>

<h3 id="others-1">Others</h3>
<h4 id="background-matting-the-world-is-your-green-screen">Background Matting: The World is Your Green Screen</h4>
<p><img src="https://camo.githubusercontent.com/89ad795b21ae7c739811372739e53985b1e7feab/68747470733a2f2f686f6d65732e63732e77617368696e67746f6e2e6564752f7e736f756d796139312f70617065725f7468756d626e61696c732f6d617474696e672e706e67 | width=250" alt="" /></p>
<ul>
  <li><a href="http://grail.cs.washington.edu/projects/background-matting/">Project Page</a></li>
  <li><a href="https://github.com/senguptaumd/Background-Matting">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.00626">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{BMSengupta20,
  title={Background Matting: The World is Your Green Screen},
  author = {Soumyadip Sengupta and Vivek Jayaram and Brian Curless and Steve Seitz and Ira Kemelmacher-Shlizerman},
  booktitle={Computer Vision and Pattern Regognition (CVPR)},
  year={2020}
}
</code></pre></div></div>

<ul>
  <li><a href="https://link.medium.com/suCEIzEed7">Blog Post</a></li>
</ul>

<h4 id="3d-photography-using-context-aware-layered-depth-inpainting-cvpr-2020">3D Photography using Context-aware Layered Depth Inpainting (CVPR 2020)</h4>
<p><img src="https://camo.githubusercontent.com/8dd5b529c99cdfcedd043c8239b68c4d7a23a148/68747470733a2f2f66696c65626f782e6563652e76742e6564752f7e6a626875616e672f70726f6a6563742f334450686f746f2f334450686f746f5f7465617365722e6a7067 | width=250" alt="" /></p>
<ul>
  <li><a href="https://shihmengli.github.io/3D-Photo-Inpainting/">Project Page</a></li>
  <li><a href="https://github.com/vt-vl-lab/3d-photo-inpainting/blob/master/README.md">Code</a></li>
  <li><a href="https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz">Colab</a></li>
  <li><a href="https://arxiv.org/abs/2004.04727">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Shih3DP20,
  author = {Shih, Meng-Li and Su, Shih-Yang and Kopf, Johannes and Huang, Jia-Bin},
  title = {3D Photography using Context-aware Layered Depth Inpainting},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2020}
}
</code></pre></div></div>

<h4 id="project-an-image-centroid-to-another-image-using-opencv">Project an image centroid to another image using OpenCV</h4>
<p><img src="https://github.com/cyrildiagne/screenpoint/blob/master/example/match_debug.png?raw=true" alt="https://github.com/cyrildiagne/screenpoint/blob/master/example/match_debug.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/cyrildiagne/screenpoint/blob/master/README.md">Code</a></li>
</ul>

<h3 id="image-to-text">Image to Text</h3>

<h5 id="compguesswhat-a-multi-task-evaluation-framework-for-grounded-language-learning">CompGuessWhat?!: a Multi-Task Evaluation Framework for Grounded Language Learning</h5>
<p><img src="https://compguesswhat.github.io/img/attribute_prediction.png" alt="" /></p>
<ul>
  <li><a href="https://github.com/CompGuessWhat">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.02174">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{suglia2020compguesswhat,
  title={CompGuessWhat?!: a Multi-task Evaluation Framework for Grounded Language Learning},
  author={Suglia, Alessandro, Konstas, Ioannis, Vanzo, Andrea, Bastianelli, Emanuele, Desmond Elliott, Stella Frank and Oliver Lemon},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}
</code></pre></div></div>

<p>https://compguesswhat.github.io/paper/</p>
<h5 id="yolov4-optimal-speed-and-accuracy-of-object-detection">YOLOv4: Optimal Speed and Accuracy of Object Detection</h5>
<p><img src="https://i.ibb.co/mz376Rd/Image-PNG.png" alt="" /></p>
<ul>
  <li><a href="https://github.com/AlexeyAB/darknet">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.10934">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{bochkovskiy2020yolov4,
    title={YOLOv4: Optimal Speed and Accuracy of Object Detection},
    author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
    year={2020},
    eprint={2004.10934},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<h5 id="image-captioning-with-pytorch">Image Captioning with PyTorch</h5>
<p><img src="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg | width=250" alt="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg" /></p>
<ul>
  <li><a href="https://github.com/jayeshsaita/image_captioning_pytorch">Code</a></li>
</ul>

<h5 id="resnest-split-attention-networks">ResNeSt: Split-Attention Networks</h5>
<p><img src="https://raw.githubusercontent.com/zhanghang1989/ResNeSt/master/miscs/abstract.jpg" alt="" /></p>
<ul>
  <li><a href="https://github.com/zhanghang1989/ResNeSt">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.08955">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zhang2020resnest,
title={ResNeSt: Split-Attention Networks},
author={Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Zhang, Zhi and Lin, Haibin and Sun, Yue and He, Tong and Muller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
journal={arXiv preprint arXiv:2004.08955},
year={2020}
}
</code></pre></div></div>

<h5 id="hands-on-guide-to-sign-language-classification-using-cnn">Hands-on guide to sign language classification using CNN</h5>
<p><a href="https://analyticsindiamag.com/hands-on-guide-to-sign-language-classification-using-cnn/">Hands-on guide to sign language classification using CNN</a></p>

<h3 id="image-to-soundspeech">Image to Sound/Speech</h3>

<h3 id="image-to-video">Image to Video</h3>

<h2 id="text-to-anything">Text to Anything</h2>

<h3 id="text-to-image">Text to Image</h3>
<h4 id="network-fusion-for-content-creation-with-conditional-inns-cvpr-2020">Network Fusion for Content Creation with Conditional INNs (CVPR 2020)</h4>
<ul>
  <li><a href="https://compvis.github.io/network-fusion/">Project Page</a></li>
  <li><a href="https://arxiv.org/abs/2005.13580">Paper</a></li>
</ul>

<h3 id="text-to-text">Text to Text</h3>

<h4 id="bilingual-translation">Bilingual Translation</h4>
<ul>
  <li><a href="https://gist.github.com/sshleifer/a5498e4d829a016b5875516d659ed50f">Example</a></li>
  <li><a href="https://github.com/sshleifer/marian">Code</a></li>
  <li><a href="https://github.com/sshleifer/marian">Author</a></li>
</ul>

<h4 id="t5-finetuning">T5 finetuning</h4>
<ul>
  <li><a href="https://arxiv.org/abs/1910.10683">Paper</a></li>
</ul>

<h4 id="training-electra">Training Electra</h4>
<ul>
  <li>[Pre-train ELECTRA from Scratch for Spanish] (https://chriskhanhtran.github.io/posts/electra-spanish/)</li>
</ul>

<h4 id="text-translation">Text Translation</h4>
<ul>
  <li><a href="https://towardsdatascience.com/build-your-own-machine-translation-service-with-transformers-d0709df0791b">Blog</a></li>
</ul>

<h4 id="text-generation">Text Generation</h4>
<h5 id="lyrics-generation">Lyrics Generation</h5>
<ul>
  <li><a href="https://colab.research.google.com/drive/12g07FS2WkNctNy_bYb7a5ZNFAsJcN0dz?usp=sharing">Colab</a></li>
  <li><a href="https://eilab.gatech.edu/mark-riedl">Author</a></li>
</ul>

<h5 id="next-word-prediction">Next Word Prediction</h5>
<p><img src="https://raw.githubusercontent.com/renatoviolin/next_word_prediction/master/word_prediction.gif =250x" alt="UI" /></p>
<ul>
  <li><a href="https://github.com/renatoviolin/next_word_prediction">Code</a></li>
</ul>

<h4 id="code-to-code">Code to Code</h4>
<h5 id="unsupervised-translation-of-programming-languages">Unsupervised Translation of Programming Languages</h5>
<ul>
  <li><a href="https://arxiv.org/abs/2006.03511">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{lachaux2020unsupervised,
    title={Unsupervised Translation of Programming Languages},
    author={Marie-Anne Lachaux and Baptiste Roziere and Lowik Chanussot and Guillaume Lample},
    year={2020},
    eprint={2006.03511},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/xTzFJIknh7E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="text-to-soundspeech">Text to Sound/Speech</h3>
<h4 id="pitchtron-towards-audiobook-generation-from-ordinary-peoples-voices">Pitchtron: Towards audiobook generation from ordinary people’s voices</h4>
<ul>
  <li><a href="https://sunghee.kaist.ac.kr/entry/pitchtron">Project Page</a></li>
  <li><a href="https://arxiv.org/abs/2005.10456">Paper</a></li>
  <li><a href="https://github.com/hash2430/pitchtron">Code</a></li>
  <li><a href="https://sunghee.kaist.ac.kr/entry/pitchtron">Samples</a></li>
</ul>

<audio controls="controls">
    <source src="https://www.dropbox.com/s/g9ch0aw2wbgmwek/hard_emotive_1.wav?dl=1" type="audio/wav" />&lt;/source&gt;
    <p>Your browser does not support the audio element.</p>
</audio>

<h4 id="transformers-tts">Transformers TTS</h4>
<ul>
  <li><a href="https://as-ideas.github.io/TransformerTTS/">Project Page</a></li>
  <li><a href="https://github.com/as-ideas/TransformerTTS">Code</a></li>
  <li><a href="https://as-ideas.github.io/TransformerTTS/">Samples</a></li>
</ul>

<audio controls="controls">
    <source src="https://github.com/as-ideas/tts_model_outputs/blob/master/ljspeech_transformertts/Trump.wav?raw=true" type="audio/wav" />&lt;/source&gt;
    <p>Your browser does not support the audio element.</p>
</audio>

<h3 id="text-to-video">Text to Video</h3>

<h2 id="soundspeech-to-anything">Sound/Speech to Anything</h2>

<h3 id="soundspeech-to-image">Sound/Speech to Image</h3>
<h5 id="audio-to-image-conversion">Audio to Image Conversion</h5>
<ul>
  <li><a href="https://www.kaggle.com/timolee/audio-data-conversion-to-images-eda">Code</a></li>
</ul>

<h3 id="soundspeech-to-text">Sound/Speech to Text</h3>
<h5 id="speech-command-recognition">Speech Command Recognition</h5>
<ul>
  <li><a href="https://github.com/jayeshsaita/Speech-Commands-Recognition">Code</a></li>
  <li><a href="https://www.linkedin.com/in/jayeshsaita">Author</a></li>
</ul>

<h3 id="soundspeech-to-soundspeech">Sound/Speech to Sound/Speech</h3>
<h4 id="speaker-independent-emotional-voice-conversion-based-on-conditional-vaw-gan-and-cwt">Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT</h4>
<ul>
  <li><a href="https://kunzhou9646.github.io/speaker-independent-emotional-vc/">Project Page</a></li>
  <li><a href="https://github.com/KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT">Code</a></li>
  <li><a href="https://www.researchgate.net/publication/341388058_Converting_Anyone's_Emotion_Towards_Speaker-Independent_Emotional_Voice_Conversion">Paper</a></li>
  <li><a href="https://kunzhou9646.github.io/speaker-independent-emotional-vc/">Sample</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@unknown{unknown,
author = {Zhou, Kun and Sisman, Berrak and Zhang, Mingyang and Li, Haizhou},
year = {2020},
month = {05},
pages = {},
title = {Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion},
doi = {10.13140/RG.2.2.20921.60006}
}
</code></pre></div></div>

<h3 id="soundspeech-to-video">Sound/Speech to Video</h3>

<h2 id="video-to-anything">Video to Anything</h2>

<h3 id="video-to-video">Video to Video</h3>
<h4 id="segmentation-1">Segmentation</h4>
<h5 id="mseg--a-composite-dataset-for-multi-domain-semantic-segmentation">MSeg : A Composite Dataset for Multi-domain Semantic Segmentation</h5>
<p><img src="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif | width=250" alt="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif" /></p>
<ul>
  <li><a href="https://github.com/mseg-dataset">Code</a></li>
  <li><a href="https://vladlen.info/papers/MSeg.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{MSeg_2020_CVPR,
author = {Lambert, John and Zhuang, Liu and Sener, Ozan and Hays, James and Koltun, Vladlen},
title = {MSeg A Composite Dataset for Multi-domain Semantic Segmentation},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
year = {2020}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/PzBK6K5gyyo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="motion-supervised-co-part-segmentation">Motion Supervised co-part Segmentation</h5>
<p><img src="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true | width=250" alt="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true" /></p>
<ul>
  <li><a href="https://github.com/AliaksandrSiarohin/motion-cosegmentation">Code</a></li>
  <li><a href="http://arxiv.org/abs/2004.03234">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Siarohin_2020_motion,
  title={Motion Supervised co-part Segmentation},
  author={Siarohin, Aliaksandr and Roy, Subhankar and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={arXiv preprint},
  year={2020}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/RJ4Nj1wV5iA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="video-to-image">Video to Image</h3>

<h3 id="video-to-text">Video to Text</h3>

<h3 id="video-tosoundspeech">Video toSound/Speech</h3>

<h3 id="video-to-video-1">Video to Video</h3>

<h1 id="inference">Inference</h1>
<h2 id="python-serving">Python serving</h2>
<ul>
  <li><a href="https://towardsdatascience.com/why-we-switched-from-flask-to-fastapi-for-production-machine-learning-765aab9b3679">Why we switched from Flask to FastAPI for production machine learning
</a></li>
</ul>

<h2 id="fastai">Fastai</h2>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce <a href="https://twitter.com/hashtag/fastinference?src=hash&amp;ref_src=twsrc%5Etfw">#fastinference</a>! The bad news: fastshap and ClassConfusion are gone. The good news: they moved to a new home! In this module we have all of the above plus some speed-up and QOL integrations into <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a>&#39;s inference methods, see here: 1/<a href="https://t.co/SLgJahtSr5">https://t.co/SLgJahtSr5</a> <a href="https://t.co/1oFkMe4SsP">pic.twitter.com/1oFkMe4SsP</a></p>&mdash; Zach Mueller (@TheZachMueller) <a href="https://twitter.com/TheZachMueller/status/1269818072577331200?ref_src=twsrc%5Etfw">June 8, 2020</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p><a href="https://jianjye.com/p/deploy-fastai-digitalocean-ubuntu-flask-supervisor/">How to deploy Fastai on Ubuntu</a></p>

<h2 id="huggingface">HuggingFace</h2>
<ul>
  <li><a href="https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18">Deploying HuggingFace to Production</a></li>
  <li><a href="https://github.com/huggingface/transformers/blob/master/examples/movement-pruning/README.md">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{sanh2020movement,
    title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
    author={Victor Sanh and Thomas Wolf and Alexander M. Rush},
    year={2020},
    eprint={2005.07683},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<h2 id="hummingbird">Hummingbird</h2>
<p>python library that compiles trained ML models into tensor computation for faster inference. Supported models include sklearn decision trees, random forest, lightgbm, xgboost.</p>

<ul>
  <li><a href="https://github.com/microsoft/hummingbird">Code</a></li>
  <li><a href="https://github.com/microsoft/hummingbird/tree/master/notebooks">Examples</a></li>
</ul>

<h1 id="tools">Tools</h1>
<h2 id="terminal">Terminal</h2>
<h3 id="rich">Rich</h3>
<p>Rich is a Python library for rich text and beautiful formatting in the terminal
<img src="https://github.com/willmcgugan/rich/raw/master/imgs/features.png?raw=true" alt="https://github.com/willmcgugan/rich/raw/master/imgs/features.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/willmcgugan/rich">Code</a></li>
  <li><a href="https://rich.readthedocs.io/en/latest/">Doc</a></li>
</ul>

<h2 id="python">Python</h2>
<h3 id="pyaudio-fft">PyAudio FFT</h3>
<p><img src="https://raw.githubusercontent.com/tr1pzz/Realtime_PyAudio_FFT/master/assets/teaser.gif" alt="https://raw.githubusercontent.com/tr1pzz/Realtime_PyAudio_FFT/master/assets/teaser.gif" /></p>
<ul>
  <li><a href="https://github.com/tr1pzz/Realtime_PyAudio_FFT">https://github.com/tr1pzz/Realtime_PyAudio_FFT</a></li>
</ul>

<h3 id="process-mining--alpha-miner">Process Mining : alpha-miner</h3>
<ul>
  <li><a href="http://pm4py.pads.rwth-aachen.de/documentation/process-discovery/alpha-miner/">Project Page</a></li>
</ul>

<h3 id="image-feature-extractor">Image Feature extractor</h3>
<ul>
  <li><a href="https://github.com/aleSuglia/image-feature-extractors/blob/master/README.md">Code</a></li>
</ul>

<h1 id="cool-projects">Cool projects</h1>
<h2 id="web-based-training">Web based Training</h2>
<ul>
  <li><a href="https://teachablemachine.withgoogle.com/">Teachable Machine</a></li>
</ul>

<h2 id="how-to-evaluate-longformer-on-triviaqa-using-nlp">How to evaluate Longformer on TriviaQA using NLP</h2>
<ul>
  <li><a href="https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing#scrollTo=lbNZdYkugq7-">Colab</a></li>
</ul>

<h2 id="data-visualization">Data Visualization</h2>
<ul>
  <li><a href="https://medium.com/nightingale/choosing-a-font-for-your-data-visualization-2ed37afea637">Choosing Fonts for Your Data Visualization</a></li>
</ul>

<h1 id="hardware">Hardware</h1>
<h2 id="gpu">GPU</h2>
<h3 id="nvidia">Nvidia</h3>
<h4 id="ampere">Ampere</h4>
<ul>
  <li><a href="https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/">NVIDIA Ampere Architecture In-Depth</a></li>
</ul>

<h1 id="mooc">MOOC</h1>
<h2 id="fastai-1">Fast.ai</h2>
<ul>
  <li><a href="https://www.fast.ai/">Image</a></li>
  <li><a href="https://www.fast.ai/2019/07/08/fastai-nlp/">NLP</a></li>
</ul>

<h2 id="benchmark">Benchmark</h2>
<h3 id="nlp-1">NLP</h3>
<p><img src="https://airev.us/wp-content/uploads/2020/05/tabela-duz%CC%87a_pop-1024x1536.png" alt="" /></p>
<ul>
  <li><a href="https://airev.us/ultimate-guide-to-natural-language-processing-courses">Full Blog Post</a></li>
  <li><a href="https://www.fast.ai/2019/07/08/fastai-nlp/">Missing in Blog Post</a></li>
</ul>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jqueguiner" title="jqueguiner"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://stackoverflow.com/users/users%2F786021%2Fguignol" title="users/786021/guignol"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/jlqueguiner" title="jlqueguiner"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jilijeanlouis" title="jilijeanlouis"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a rel="me" href="https://www.youtube.com/UCwPhv5c2FQf6Ai3E6rT8_cw" title="UCwPhv5c2FQf6Ai3E6rT8_cw"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
